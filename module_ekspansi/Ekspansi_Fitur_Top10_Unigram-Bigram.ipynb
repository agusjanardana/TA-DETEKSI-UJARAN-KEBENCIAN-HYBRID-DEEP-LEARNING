{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FEATURE EXPANSION WITH TF-IDF\n",
    "Copyright @I Gde Bagus Janardana Abasan\n",
    "Telkom University\n",
    "1301190061"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class FeatureExpansion:\n",
    "    def __init__(self, dataset, corpus_fasttext_nrank):\n",
    "        self.features_v_tfidf = None\n",
    "        self.vector_tfidf = None\n",
    "        self.tf_idf_data = None\n",
    "        self.dataset = dataset\n",
    "        self.vector = []\n",
    "        self.features_v = []\n",
    "        self.data_list_tokenized = list(self.dataset['preprocess_token'])\n",
    "        self.corpus_fasttext_nrank = corpus_fasttext_nrank\n",
    "        self.dictionary_combined_fasttext = None\n",
    "        self.fe_vec = []\n",
    "        self.tf_idf_df = pd.DataFrame()\n",
    "        self.tf_idf_vec = []\n",
    "        self.binary_vectorizer = None\n",
    "        self.binary_vectorizer_dataframe = []\n",
    "        self.fe_BINARYVEC_df = pd.DataFrame()\n",
    "        self.tf_idf_vec = []\n",
    "\n",
    "    def datasetToBinaryVector(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(binary=True, ngram_range=(1,2), max_features=10000)\n",
    "        self.binary_vectorizer = vectorizer.fit_transform(self.dataset[\"preprocess_final\"])\n",
    "        self.binary_vectorizer_dataframe  = pd.DataFrame(self.binary_vectorizer.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "        self.features_v = vectorizer.get_feature_names_out()\n",
    "        self.vector = self.binary_vectorizer_dataframe.values.tolist()\n",
    "        self.dataset['Binary_vec'] = self.vector\n",
    "\n",
    "        return self.dataset\n",
    "\n",
    "    def binaryVectorToTFIDF(self, filename):\n",
    "\n",
    "        #define tf-idf\n",
    "        self.tf_idf_vec = TfidfTransformer()\n",
    "\n",
    "        #transform\n",
    "        self.tf_idf_data = self.tf_idf_vec.fit_transform(self.binary_vectorizer)\n",
    "\n",
    "        #create dataframe\n",
    "        self.tf_idf_df  = pd.DataFrame(self.tf_idf_data.toarray(),columns=self.features_v)\n",
    "\n",
    "        self.tf_idf_df.to_csv(filename, index=False)\n",
    "\n",
    "        return self.tf_idf_df\n",
    "\n",
    "    def buildCombinedCorpusDictionary(self, rank):\n",
    "        \"\"\"\n",
    "        Untuk pembuatan combined corpus fasttext dictionary yang berisi kata-kata yang ada di corpus fasttext\n",
    "        :param rank:\n",
    "        \"\"\"\n",
    "        match rank:\n",
    "            case 1:\n",
    "                self.corpus_fasttext_nrank['combined_top1'] = self.corpus_fasttext_nrank.apply(lambda x: list([x['Rank 1']]),axis=1)\n",
    "                self.dictionary_combined_fasttext = dict((Words, combined) for Words, combined in zip(self.corpus_fasttext_nrank.Words, self.corpus_fasttext_nrank.combined_top1))\n",
    "            case 5:\n",
    "                self.corpus_fasttext_nrank['combined_top5'] = self.corpus_fasttext_nrank.apply(lambda x: list([x['Rank 1'], x['Rank 2'], x['Rank 3'], x['Rank 4'], x['Rank 5']]),axis=1)\n",
    "                self.dictionary_combined_fasttext = dict((Words, combined) for Words, combined in zip(self.corpus_fasttext_nrank.Words, self.corpus_fasttext_nrank.combined_top5))\n",
    "            case 10:\n",
    "                self.corpus_fasttext_nrank['combined_top10'] = self.corpus_fasttext_nrank.apply(lambda x: list([x['Rank 1'], x['Rank 2'], x['Rank 3'], x['Rank 4'], x['Rank 5'],  x['Rank 6'], x['Rank 7'], x['Rank 8'], x['Rank 9'], x['Rank 10']]),axis=1)\n",
    "                self.dictionary_combined_fasttext = dict((Words, combined_top10) for Words, combined_top10 in zip(self.corpus_fasttext_nrank.Words, self.corpus_fasttext_nrank.combined_top10))\n",
    "\n",
    "\n",
    "    def checkWords(self, i, j):\n",
    "        \"\"\"\n",
    "        Untuk pengecekan apakah kata-kata yang ada di corpus fasttext ada di corpus T\n",
    "        :param i adalah index dari self.vector:\n",
    "        :param j adalah index dari self.vector[i] ibaratnya vector[i][j]:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # pengecekan if one word in W appears in corpus T fasttext\n",
    "            a = [t for t in self.dictionary_combined_fasttext[self.features_v[j]] if t in self.data_list_tokenized[i]]\n",
    "            return a\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def weightCheck(self, w, i, j):\n",
    "        \"\"\"\n",
    "        Untuk pengecekan weight tf-idf dari kata-kata yang ada di corpus fasttext\n",
    "        :param w:\n",
    "        :param i:\n",
    "        :param j:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if w != []:\n",
    "                idx = np.where(self.features_v == w[0])[0][0]\n",
    "                if self.vector[i][idx] != 0:\n",
    "                    print(f'kata {self.features_v[j]} memiliki arti identik dengan kata {w[0]} sehingga nilai vektor di ubah menjadi {self.vector[i][idx]}')\n",
    "                return self.vector[i][idx]\n",
    "            else:\n",
    "                return self.vector[i][j]\n",
    "        except:\n",
    "            return self.vector[i][j]\n",
    "\n",
    "\n",
    "    def callFeatureExpansion(self):\n",
    "        print()\n",
    "        print('============ FEATURE EXPANSION IS PROCESSING ============')\n",
    "        binary_matrix = self.binary_vectorizer.toarray()\n",
    "        for i in range(len(self.vector)):\n",
    "            v = []\n",
    "            for j in range(len(self.vector[i])):\n",
    "                if self.vector[i][j] == 0:\n",
    "                    expanded_value = [self.weightCheck(self.checkWords(i,j), i, j)]\n",
    "                    v.extend(expanded_value)  # tambahkan setiap fitur yang dihasilkan ke dalam vektor\n",
    "                else:\n",
    "                    v.append(self.vector[i][j])\n",
    "            self.fe_vec.append(v)\n",
    "            self.binary_vectorizer = csr_matrix(self.fe_vec)\n",
    "        self.dataset['BinaryVector_FE_VEC'] = self.fe_vec\n",
    "        # update binary_vectorizer\n",
    "        self.fe_BINARYVEC_df = pd.DataFrame(self.fe_vec, columns=self.features_v)\n",
    "        return self.fe_BINARYVEC_df\n",
    "\n",
    "    def checkingFeatureExpansionResult(self):\n",
    "        lis1 = list(self.dataset['Binary_vec'])\n",
    "        lis2 = list(self.dataset['BinaryVector_FE_VEC'])\n",
    "        if lis1 != lis2:\n",
    "            print('Binary_vec dan BinaryVector_FE_VEC tidak sama, jadi feature expansion berjalan')\n",
    "\n",
    "        print(len(self.fe_vec[0]), len(self.vector[0]))\n",
    "        print('same?', (self.fe_vec == self.vector))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERITA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape berita : (478665, 11)\n"
     ]
    }
   ],
   "source": [
    "# LOAD CORPUS\n",
    "corpus_similarity_berita = pd.read_csv('../data/data_preprocessed/corpus_fasttext_topnrank/berita/df_similarity_top10_unigram_bigram.csv')\n",
    "print(f'shape berita :', corpus_similarity_berita.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape dataset : (49841, 11)\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATASET\n",
    "dataset_hatespeech = pd.read_csv('../data/data_preprocessed/dataset/DatasetHateSpeech_Final_TA2023.csv')\n",
    "print(f'shape dataset :', dataset_hatespeech.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m expansion \u001B[38;5;241m=\u001B[39m FeatureExpansion(dataset_hatespeech, corpus_similarity_berita)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# lakukan TF-IDF vectorize terhadap dataset\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m df_countVectorizer \u001B[38;5;241m=\u001B[39m \u001B[43mexpansion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdatasetToBinaryVector\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m df_countVectorizer[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBinary_vec\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "Cell \u001B[1;32mIn[2], line 27\u001B[0m, in \u001B[0;36mFeatureExpansion.datasetToBinaryVector\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary_vectorizer_dataframe  \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary_vectorizer\u001B[38;5;241m.\u001B[39mtoarray(),columns\u001B[38;5;241m=\u001B[39mvectorizer\u001B[38;5;241m.\u001B[39mget_feature_names_out())\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_v \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mget_feature_names_out()\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvector \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_vectorizer_dataframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBinary_vec\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvector\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\n",
      "\u001B[1;31mMemoryError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "expansion = FeatureExpansion(dataset_hatespeech, corpus_similarity_berita)\n",
    "# lakukan TF-IDF vectorize terhadap dataset\n",
    "df_countVectorizer = expansion.datasetToBinaryVector()\n",
    "df_countVectorizer['Binary_vec']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#build corpus fasttext dictionary top10\n",
    "expansion.buildCombinedCorpusDictionary(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lakukan feature expansion\n",
    "df_fe = expansion.callFeatureExpansion()\n",
    "df_fe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transfor to tf-idf\n",
    "df_tfidf = expansion.binaryVectorToTFIDF('../data/data_preprocessed/hasil_ekspansi/berita/DatasetHatespeech_UnigramBigram_Top10_ExpandedBerita.csv')\n",
    "df_tfidf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TWEET"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LOAD CORPUS\n",
    "corpus_similarity_tweet = pd.read_csv('../data/data_preprocessed/corpus_fasttext_topnrank/tweet/df_similarity_top10_unigram_bigram.csv')\n",
    "print(f'shape berita :', corpus_similarity_tweet.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "dataset_hatespeech = pd.read_csv('../data/data_preprocessed/dataset/DatasetHateSpeech_Final_TA2023.csv')\n",
    "print(f'shape dataset :', dataset_hatespeech.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "expansion = FeatureExpansion(dataset_hatespeech, corpus_similarity_tweet)\n",
    "# lakukan TF-IDF vectorize terhadap dataset\n",
    "df_countVectorizer = expansion.datasetToBinaryVector()\n",
    "df_countVectorizer['Binary_vec']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#build corpus fasttext dictionary top10\n",
    "expansion.buildCombinedCorpusDictionary(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lakukan feature expansion\n",
    "df_fe = expansion.callFeatureExpansion()\n",
    "df_fe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transfor to tf-idf\n",
    "df_tfidf = expansion.binaryVectorToTFIDF('../data/data_preprocessed/hasil_ekspansi/twitter/DatasetHatespeech_UnigramBigram_Top10_ExpandedTwitter.csv')\n",
    "df_tfidf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TWEET-BERITA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LOAD CORPUS\n",
    "corpus_similarity_berita = pd.read_csv('../data/data_preprocessed/corpus_fasttext_topnrank/berita/df_similarity_top10_unigram_bigram.csv')\n",
    "corpus_similarity_tweet = pd.read_csv('../data/data_preprocessed/corpus_fasttext_topnrank/tweet/df_similarity_top10_unigram_bigram.csv')\n",
    "\n",
    "corpus_similarity_merged = pd.concat([corpus_similarity_berita,corpus_similarity_tweet])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "dataset_hatespeech = pd.read_csv('../data/data_preprocessed/dataset/DatasetHateSpeech_Final_TA2023.csv')\n",
    "print(f'shape dataset :', dataset_hatespeech.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "expansion = FeatureExpansion(dataset_hatespeech, corpus_similarity_merged)\n",
    "# lakukan TF-IDF vectorize terhadap dataset\n",
    "df_countVectorizer = expansion.datasetToBinaryVector()\n",
    "df_countVectorizer['Binary_vec']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#build corpus fasttext dictionary top1\n",
    "expansion.buildCombinedCorpusDictionary(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lakukan feature expansion\n",
    "df_fe = expansion.callFeatureExpansion()\n",
    "df_fe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transfor to tf-idf\n",
    "df_tfidf = expansion.binaryVectorToTFIDF('../data/data_preprocessed/hasil_ekspansi/twitterberita/DatasetHatespeech_UnigramBigram_Top10_ExpandedBeritaTwitter.csv')\n",
    "df_tfidf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
